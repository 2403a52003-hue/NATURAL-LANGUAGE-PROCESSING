{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#STEP 1: Install and Import Everything (Run first cell)\n",
        "!pip install -U nltk pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocVJiDDR0knI",
        "outputId": "559f34b0-2c23-42d9-aadc-fab41bb1f7bf"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (3.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import sys\n",
        "\n",
        "# Clear any previously loaded NLTK modules from sys.modules to force a fresh import\n",
        "for module_name in list(sys.modules.keys()):\n",
        "    if module_name.startswith('nltk'):\n",
        "        try:\n",
        "            del sys.modules[module_name]\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab') # Added to download the missing resource\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Added to download the specific English tagger\n",
        "\n",
        "print(\"Setup complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBaSt71N1GZA",
        "outputId": "4fd89a66-8654-40ce-88ea-444c581c5fe7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 2: Load Dataset Safely (works even if column name differs)\n",
        "data = pd.read_csv(\"/content/Twitter_Data.csv\")\n",
        "\n",
        "print(\"Columns in dataset:\", data.columns)\n",
        "\n",
        "# Take first column automatically (safe method)\n",
        "tweets = data.iloc[:, 0].dropna().head(200)\n",
        "\n",
        "print(\"Sample tweets:\")\n",
        "print(tweets.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_jqvZYu1GUQ",
        "outputId": "a55bf673-8838-484a-b251-6683d82b4fb1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in dataset: Index(['clean_text', 'category'], dtype='object')\n",
            "Sample tweets:\n",
            "0    when modi promised “minimum government maximum...\n",
            "1    talk all the nonsense and continue all the dra...\n",
            "2    what did just say vote for modi  welcome bjp t...\n",
            "3    asking his supporters prefix chowkidar their n...\n",
            "4    answer who among these the most powerful world...\n",
            "Name: clean_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 3: Preprocess Tweets (no errors)\n",
        "def clean_tweet(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)\n",
        "    text = re.sub(r\"#\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "cleaned = tweets.apply(clean_tweet)\n",
        "\n",
        "print(\"\\nCleaned tweets:\")\n",
        "print(cleaned.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R63qZYEP1lBi",
        "outputId": "a8756dae-a38c-4bf6-ac05-f2b90b7cdef7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cleaned tweets:\n",
            "0    when modi promised “minimum government maximum...\n",
            "1    talk all the nonsense and continue all the dra...\n",
            "2    what did just say vote for modi  welcome bjp t...\n",
            "3    asking his supporters prefix chowkidar their n...\n",
            "4    answer who among these the most powerful world...\n",
            "Name: clean_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 4: POS Tagging using NLTK (guaranteed working)\n",
        "tagged_sentences = []\n",
        "\n",
        "for tweet in cleaned:\n",
        "    tokens = nltk.word_tokenize(tweet)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    tagged_sentences.append(tagged)\n",
        "\n",
        "print(\"\\nFirst POS-tagged tweet:\")\n",
        "print(tagged_sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4wUc7-p10ry",
        "outputId": "b1429dd2-aeda-45f0-f66d-0d31ad1afd8d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First POS-tagged tweet:\n",
            "[('when', 'WRB'), ('modi', 'NN'), ('promised', 'VBD'), ('“', 'NNP'), ('minimum', 'JJ'), ('government', 'NN'), ('maximum', 'JJ'), ('governance', 'NN'), ('”', 'NNP'), ('expected', 'VBD'), ('him', 'PRP'), ('begin', 'VB'), ('the', 'DT'), ('difficult', 'JJ'), ('job', 'NN'), ('reforming', 'VBG'), ('the', 'DT'), ('state', 'NN'), ('why', 'WRB'), ('does', 'VBZ'), ('take', 'VB'), ('years', 'NNS'), ('get', 'VB'), ('justice', 'NN'), ('state', 'NN'), ('should', 'MD'), ('and', 'CC'), ('not', 'RB'), ('business', 'NN'), ('and', 'CC'), ('should', 'MD'), ('exit', 'VB'), ('psus', 'NN'), ('and', 'CC'), ('temples', 'NNS')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 5: Build HMM Transition & Emission Counts\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "transition = defaultdict(Counter)\n",
        "emission = defaultdict(Counter)\n",
        "\n",
        "for sent in tagged_sentences:\n",
        "    prev_tag = \"<s>\"\n",
        "    for word, tag in sent:\n",
        "        transition[prev_tag][tag] += 1\n",
        "        emission[tag][word.lower()] += 1\n",
        "        prev_tag = tag\n",
        "\n",
        "print(\"\\nSample Transition Probabilities:\")\n",
        "print(dict(list(transition.items())[:3]))\n",
        "\n",
        "print(\"\\nSample Emission Probabilities:\")\n",
        "print(dict(list(emission.items())[:3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG3qvFAL2Ni9",
        "outputId": "6b37941a-dca9-4ba7-edcc-d26c91c21014"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Transition Probabilities:\n",
            "{'<s>': Counter({'NN': 76, 'JJ': 22, 'RB': 20, 'DT': 12, 'IN': 12, 'NNS': 11, 'WRB': 9, 'VBG': 7, 'VB': 4, 'VBN': 4, 'WP': 3, 'CD': 3, 'VBD': 3, 'PRP$': 3, 'MD': 3, 'VBZ': 2, 'PRP': 2, 'VBP': 2, 'JJR': 1, 'CC': 1}), 'WRB': Counter({'NN': 6, 'JJ': 5, 'VBP': 3, 'RB': 3, 'VBZ': 2, 'VBN': 2, 'PRP': 2, 'VB': 2, 'DT': 2, 'JJS': 1, 'NNS': 1, 'MD': 1, 'PRP$': 1}), 'NN': Counter({'NN': 390, 'IN': 113, 'JJ': 67, 'NNS': 66, 'CC': 59, 'VBD': 55, 'RB': 49, 'VBZ': 38, 'VBP': 36, 'DT': 36, 'VBG': 32, 'MD': 32, 'PRP': 22, 'WP': 13, 'VB': 13, 'NNP': 10, 'WRB': 9, 'VBN': 9, 'CD': 7, 'FW': 6, 'PRP$': 6, 'WDT': 5, 'PDT': 3, 'RP': 2, 'EX': 2, 'RBR': 1, 'WP$': 1, 'JJR': 1})}\n",
            "\n",
            "Sample Emission Probabilities:\n",
            "{'WRB': Counter({'why': 13, 'when': 8, 'how': 5, 'where': 5}), 'NN': Counter({'modi': 124, 'india': 28, 'vote': 27, 'bjp': 14, 'time': 11, 'election': 11, 'country': 11, 'congress': 9, 'government': 8, 'chowkidar': 8, 'everyone': 8, 'minister': 7, 'nation': 7, 'corruption': 7, 'state': 6, '’': 6, 'party': 6, 'family': 6, 'power': 6, 'opposition': 6, 'job': 5, 'campaign': 5, 'action': 5, 'candidate': 5, 'life': 5, 'masood': 5, 'govts': 5, 'app': 5, 'please': 5, 'way': 5, 'tweetstorm': 5, 'govt': 4, 'man': 4, 'term': 4, 'rahul': 4, 'money': 4, 'rss': 4, 'centre': 4, 'hai': 4, 'sir': 4, 'youth': 4, 'dont': 4, 'air': 4, 'house': 4, 'governance': 3, 'justice': 3, 'world': 3, 'leader': 3, 'cabinet': 3, 'gandhi': 3, 'difference': 3, 'development': 3, 'hindus': 3, 'pakistan': 3, 'azhar': 3, 'mother': 3, 'education': 3, 'religion': 3, 'support': 3, 'fight': 3, 'person': 3, 'modis': 3, 'bhi': 3, 'institution': 3, 'economy': 3, 'tweet': 3, 'adityanath': 3, 'bank': 3, 'sarkar': 3, 'twitter': 3, 'chowkidhar': 3, 'chor': 3, 'name': 3, 'caste': 3, 'thing': 3, 'business': 2, 'psus': 2, 'today': 2, 'deal': 2, 'gay': 2, 'gst': 2, 'kapoor': 2, 'clarion': 2, 'call': 2, 'anupam': 2, 'appeal': 2, 'side': 2, 'gen': 2, 'leadership': 2, 'nomination': 2, 'singh': 2, 'heart': 2, 'movement': 2, 'agree': 2, 'budget': 2, 'hand': 2, 'increase': 2, 'tradition': 2, 'point': 2, 'waste': 2, 'growth': 2, 'number': 2, 'year': 2, 'vision': 2, 'problem': 2, 'hindu': 2, 'sit': 2, 'home': 2, 'use': 2, 'welfare': 2, 'delivery': 2, 'system': 2, 'simple': 2, 'nothing': 2, 'hind': 2, 'bharat': 2, 'aap': 2, 'credit': 2, 'century': 2, 'namo': 2, 'record': 2, 'nirav': 2, 'question': 2, 'yrs': 2, 'ppl': 2, 'terror': 2, 'sandip': 2, 'majority': 2, 'stop': 2, 'vijay': 2, 'judge': 2, 'manoj': 2, 'tiwari': 2, 'promise': 2, 'modiji': 2, 'hate': 2, 'dream': 2, 'wise': 2, 'someone': 2, 'chief': 2, 'look': 2, 'link': 2, 'task': 2, 'song': 2, 'work': 2, 'graffiti': 2, 'mindset': 2, 'delhi': 2, 'pass': 2, 'dancer': 2, 'bar': 2, 'malnutrition': 2, 'door': 2, 'tea': 2, 'jis': 2, 'force': 2, 'see': 2, 'talk': 1, 'nonsense': 1, 'drama': 1, 'campaigner': 1, 'service': 1, 'confusion': 1, 'answer': 1, 'putin': 1, 'kiya': 1, 'tho': 1, 'comment': 1, 'karo': 1, 'grace': 1, 'become': 1, 'smriti': 1, 'hema': 1, 'introspect': 1, 'saga': 1, 'pair': 1, 'brexit': 1, 'combination': 1, 'juicy': 1, 'demonetisation': 1, 'view': 1, 'constituency2': 1, 'hope': 1, 'tuthukudi': 1, 'benifit': 1, 'thuthukudi': 1, 'wave': 1, 'leadershipwho': 1, 'fast': 1, 'none': 1, 'play': 1, 'relation': 1, 'guru': 1, 'mind': 1, 'terrorism': 1, 'didn': 1, 'anti': 1, 'plz': 1, 'one': 1, 'hooda': 1, 'belief': 1, 'shri': 1, 'khammam': 1, 'seat': 1, 'crush': 1, 'jds': 1, 'mla': 1, 'inciting': 1, 'murder': 1, 'pradesh': 1, 'loksabha': 1, 'select': 1, 'pawan': 1, 'kumar': 1, 'pandey': 1, 'condidate': 1, 'district': 1, 'bsp': 1, 'sonbhadra': 1, 'failure': 1, 'empowerment': 1, 'paradigm': 1, 'shift': 1, 'notice': 1, 'probe': 1, 'move': 1, 'varanasi': 1, 'logic': 1, 'lalit': 1, 'tenure': 1, 'modiganga': 1, 'rejuvenation': 1, 'policy': 1, 'jumpstart': 1, 'indicator': 1, 'care': 1, 'president': 1, 'gdp': 1, 'repressive': 1, 'frustation': 1, 'ministerdisgrace': 1, 'article': 1, 'premier': 1, 'archery': 1, 'league': 1, 'survey': 1, 'indias': 1, 'enmity': 1, 'handle': 1, 'sirji': 1, 'godrej': 1, 'tata': 1, 'maid': 1, 'kalla': 1, 'idu': 1, 'bekagittu': 1, 'incompetent': 1, 'ploar': 1, 'costnobody': 1, 'hindustan': 1, 'maj': 1, 'rashtra': 1, 'watch': 1, 'win': 1, 'mein': 1, 'hona': 1, 'garv': 1, 'baat': 1, 'hogi✌': 1, 'voting': 1, 'victory': 1, 'wonder': 1, 'success': 1, 'corrupt': 1, 'law': 1, 'beg': 1, 'ibc': 1, 'place': 1, 'appoint': 1, 'labs': 1, 'fasttrack': 1, 'healthcare': 1, 'defence': 1, 'well': 1, 'build': 1, 'overpromise': 1, '–': 1, 'outcome': 1, 'approach': 1, 'ौीाोै': 1, 'surgery': 1, 'cancer': 1, 'spread': 1, 'installment': 1, 'month': 1, 'scheme': 1, 'mistry': 1, 'don': 1, 'think': 1, 'dollar': 1, 'shrewdness': 1, 'achievement': 1, 'criticise': 1, 'tax': 1, 'infra': 1, 'phobia': 1, 'itna': 1, 'fark': 1, 'honesty': 1, 'jai': 1, 'muje': 1, 'puri': 1, 'janta': 1, 'par': 1, 'vishwas': 1, 'hamre': 1, 'honge': 1, 'future': 1, 'anchor': 1, 'canvas': 1, 'journalism': 1, 'slams': 1, 'decency': 1, 'yuva': 1, 'existance': 1, 'rafel': 1, 'nature': 1, 'scammer': 1, 'nautanki': 1, 'baj': 1, 'concierge': 1, 'hearing': 1, 'anything': 1, 'learn': 1, 'minority': 1, 'lot': 1, 'bjpnda': 1, 'bengaluru': 1, 'south': 1, 'attempt': 1, 'skill': 1, 'crime': 1, 'loan': 1, 'maalya': 1, 'list': 1, 'track': 1, 'kitna': 1, 'jalte': 1, 'tum': 1, 'tbfree': 1, 'length': 1, 'reception': 1, 'sense': 1, 'akhtar': 1, 'producer': 1, 'ssingh': 1, 'row': 1, 'entertainment': 1, 'news': 1, 'express': 1, 'eat': 1, 'beef': 1, 'biryani': 1, 'asaduddin': 1, 'owaisi': 1, 'ideology': 1, 'cong': 1, 'indai': 1, 'concern': 1, 'security': 1, 'praise': 1, 'worthy': 1, 'norm': 1, 'allegation': 1, 'favor': 1, 'malya': 1, 'nonscene': 1, 'face': 1, 'haunt': 1, 'abduction': 1, 'wel': 1, 'message': 1, 'sharam': 1, 'ghotale': 1, 'bech': 1, 'tumhara': 1, 'interest': 1, 'right': 1, 'petrol': 1, 'gulf': 1, 'petta': 1, 'movie': 1, 'blog': 1, 'yesterday': 1, 'campaigning': 1, 'vande': 1, 'mataramagain': 1, 'forefront': 1, 'sabbash': 1, 'mera': 1, 'peppermit': 1, 'abvp': 1, 'amit': 1, 'shah': 1, 'faramoshi': 1, 'example': 1, 'lakhs': 1, 'indian': 1, 'humane': 1, 'kind': 1, 'pure': 1, 'honest': 1, 'imran': 1, 'saharanpur': 1, 'soninlaw': 1, 'language': 1, 'protégé': 1, 'path': 1, 'company': 1, 'head': 1, 'fear': 1, 'tsunami': 1, 'frenzy': 1, 'duty': 1, 'bcoz': 1, 'mms': 1, 'incompetency': 1, 'blatant': 1, 'misuse': 1, 'misgovernance': 1, 'overt': 1, 'mob': 1, 'loot': 1, 'kejriwal': 1, 'career': 1, 'wife': 1, 'walay': 1, 'sab': 1, 'hain': 1, 'shi': 1, 'kha': 1, 'yadav': 1, 'contest': 1, 'mai': 1, 'graduate': 1, 'hucongress': 1, 'kon': 1, 'rojgaar': 1, 'faila': 1, 'rakha': 1, 'thabhai': 1, 'create': 1, 'jobsyr': 1, 'unemployment': 1, 'whoa': 1, 'actor': 1, 'statement': 1, 'band': 1, 'brahminnon': 1, 'brahmin': 1, 'focuss': 1, 'antihindu': 1, 'persecuting': 1, 'varda': 1, 'bhai': 1, 'focus': 1, 'kar': 1, 'solidarity': 1, 'offer': 1, 'lol': 1, 'funny': 1, 'detection': 1, 'sub': 1, 'mirror': 1, 'replacement': 1, 'agent': 1, 'adnan': 1, 'sami': 1, 'endia': 1, 'swarajya': 1, 'idea': 1, 'gentleman': 1, 'conviction': 1, 'clarity': 1, 'anyone': 1, 'blood': 1, 'solution': 1, 'performance': 1, 'exception': 1, 'rule': 1, 'assertion': 1, 'liquidation': 1, 'visit': 1, 'japan': 1, 'moneyexp': 1, 'experience': 1, 'pension': 1, 'dwacra': 1, 'yuvanestam': 1, 'sung': 1, 'hurrah': 1, 'welcome': 1, 'chinook': 1, 'role': 1, 'helicopter': 1, 'urge': 1, 'fauji': 1, 'tiranga': 1, 'kamra': 1, 'chutiya': 1, 'birth': 1, 'weed': 1, 'shabana': 1, 'azmi': 1, 'biopic': 1, 'priminister': 1, 'halala': 1, 'whr': 1, 'woman': 1, 'depression': 1, 'progress': 1, 'achhe': 1, 'din': 1, 'vikas': 1, 'malaya': 1, 'etc': 1, 'chance': 1, 'history': 1, 'dynasty': 1, 'swee': 1, 'ghochu': 1, 'lotus': 1, 'soil': 1, 'comman': 1, 'poll19': 1, 'capacity': 1, 'accountability': 1, 'mode': 1, 'champion': 1, 'das': 1, 'nda': 1, 'nota': 1, 'manlike': 1, 'hitler': 1, 'mussolini': 1, 'pinochet': 1, 'stalin': 1, 'log': 1, 'rrb': 1, 'rpf': 1, 'anusaryr': 1, 'hatao': 1, 'teli': 1, 'obc': 1, 'upbringing': 1, 'gatekeeper': 1, 'politickle29': 1, 'aisaa': 1, 'aspiration': 1, 'kuch': 1, 'inspiration': 1, 'exampull': 1, 'respiration': 1, 'hindooizhm': 1, 'uploa': 1, 'youtube': 1, 'celebration': 1, 'santiniketan': 1, 'forever': 1, 'sapna': 1, 'sonia': 1, 'research': 1, 'chaiwala': 1, 'daruwali': 1, 'mallikarjun': 1, 'kharge': 1, 'gulbarga': 1, 'summer': 1, 'bhaisaab': 1, 'comparison': 1, 'quality': 1, 'hunainty': 1, 'mulsim': 1, 'kashmir': 1, 'humanity': 1, 'gov': 1, 'target': 1, 'share': 1, 'option': 1, 'tie': 1, 'super': 1, 'public': 1, 'saab': 1, 'turn': 1, 'war': 1, 'environment': 1, 'line': 1, 'topmost': 1, 'canditate': 1, 'pratima': 1, 'bhoumik': 1, 'phir': 1, 'zindabad': 1, 'thug': 1, 'mumkinhai': 1, 'amma': 1, 'hotel': 1, 'respect': 1, 'age': 1, 'arrogance': 1, 'control': 1, 'tshirt': 1, 'receipt': 1, 'disaster': 1, 'chowkidaar': 1, 'tadipar': 1, 'selling': 1, 'gali': 1, 'throw': 1, 'maya': 1, 'mamta': 1, 'bjd': 1, 'floor': 1, 'akbaruddin': 1, 'modihe': 1, 'card': 1, 'passportwant': 1, 'cow': 1, 'priyanka': 1, 'celebrity': 1, 'mentality': 1, 'team': 1, 'baap': 1, 'tell': 1, 'doc': 1, 'convenient': 1, 'avenue': 1, 'imagination': 1, 'hierarchy': 1, 'drunkard': 1, 'brother': 1, 'herself': 1, 'husband': 1, '‘': 1, 'soul': 1, 'maha': 1, 'mess': 1, 'analysis': 1, 'defeat': 1, 'rafale': 1, 'makng': 1, 'case': 1, 'purchasng': 1, 'need': 1, 'cost': 1, 'mig21': 1, 'study': 1, 'tendulkar': 1, 'amitabh': 1, 'bachchan': 1, 'visiblity': 1, 'attack': 1, 'permission': 1, 'bharun': 1, 'aal': 1, 'mallaya': 1, 'landmodi': 1, 'contract': 1, 'euro': 1, 'loyalty': 1, 'voter': 1, 'awareness': 1, 'love': 1, 'crook': 1, 'suppressedreality': 1, 'speak': 1, 'adani': 1, 'project': 1, 'jharkhand': 1, 'sector': 1, 'sez': 1}), 'VBD': Counter({'was': 17, 'did': 8, 'were': 7, 'kar': 6, 'said': 3, 'asked': 3, 'put': 2, 'ordered': 2, 'met': 2, 'created': 2, 'fed': 2, 'had': 2, 'destroyed': 2, 'made': 2, 'left': 2, 'came': 2, 'called': 2, 'promised': 1, 'expected': 1, 'told': 1, 'read': 1, 'tweeted': 1, 'shri': 1, 'worshipped': 1, 'defeated': 1, 'gave': 1, 'thrashed': 1, 'looted': 1, 'announced': 1, 'handled': 1, 'took': 1, 'happened': 1, 'scared': 1, 'confused': 1, 'sided': 1, 'brotherhood': 1, 'elected': 1, 'paid': 1, 'javed': 1, 'screwed': 1, 'recorded': 1, 'failed': 1, 'performed': 1, 'worried': 1, 'returned': 1, 'lost': 1, 'replaced': 1, 'covered': 1, 'won': 1, 'increased': 1, 'disciplined': 1, 'found': 1, 'kejriwal': 1, 'armed': 1, 'exacerbated': 1, 'needed': 1, 'gurcharan': 1, 'cut': 1, 'posted': 1, 'fared': 1, 'delhi': 1, 'offered': 1, 'accepted': 1, 'lalit': 1, 'noted': 1})}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f03e6ef"
      },
      "source": [
        "## Analyze Transition Probability Irregularities\n",
        "\n",
        "### Subtask:\n",
        "Examine the 'transition' dictionary to identify any unusual or unexpected sequences of POS tags that might indicate irregularities in the learned grammar from the social media text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42797047",
        "outputId": "a7833046-3bdd-4339-f89e-222b3915289f"
      },
      "source": [
        "transition_probabilities = defaultdict(dict)\n",
        "\n",
        "for prev_tag, next_tags_counter in transition.items():\n",
        "    total_count = sum(next_tags_counter.values())\n",
        "    for next_tag, count in next_tags_counter.items():\n",
        "        transition_probabilities[prev_tag][next_tag] = count / total_count if total_count > 0 else 0\n",
        "\n",
        "print(\"\\nCalculated Transition Probabilities (first 5 previous tags):\")\n",
        "# Sort the inner dictionaries by probability in descending order for better analysis\n",
        "for prev_tag, next_probs in list(transition_probabilities.items())[:5]:\n",
        "    sorted_next_probs = dict(sorted(next_probs.items(), key=lambda item: item[1], reverse=True))\n",
        "    print(f\"  {prev_tag}: {dict(list(sorted_next_probs.items())[:5])}\") # Print top 5 for each prev_tag\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculated Transition Probabilities (first 5 previous tags):\n",
            "  <s>: {'NN': 0.38, 'JJ': 0.11, 'RB': 0.1, 'DT': 0.06, 'IN': 0.06}\n",
            "  WRB: {'NN': 0.1935483870967742, 'JJ': 0.16129032258064516, 'VBP': 0.0967741935483871, 'RB': 0.0967741935483871, 'VBZ': 0.06451612903225806}\n",
            "  NN: {'NN': 0.3601108033240997, 'IN': 0.10433979686057249, 'JJ': 0.061865189289012, 'NNS': 0.060941828254847646, 'CC': 0.05447830101569714}\n",
            "  VBD: {'JJ': 0.19469026548672566, 'NN': 0.19469026548672566, 'DT': 0.07964601769911504, 'RB': 0.061946902654867256, 'IN': 0.05309734513274336}\n",
            "  NNP: {'NN': 0.5625, 'JJ': 0.125, 'RB': 0.125, 'VBD': 0.0625, 'NNS': 0.0625}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b1ebe1c"
      },
      "source": [
        "### Analysis of Transition Probability Irregularities\n",
        "\n",
        "Upon examining the calculated transition probabilities, several patterns emerge that reflect the nature of social media text, often deviating from standard English grammar. Here are some observations:\n",
        "\n",
        "1.  **`<s>` (Start of Sentence) Transitioning Directly to Nouns (`NN`) or Adjectives (`JJ`):**\n",
        "    *   `<s>`: `{'NN': 0.38, 'JJ': 0.11, 'RB': 0.1, 'DT': 0.06, 'IN': 0.06}`\n",
        "    *   A high probability of a sentence starting directly with a Noun (NN, 38%) or Adjective (JJ, 11%) is common in headlines, short phrases, or sentences where articles/pronouns are omitted for brevity, characteristic of social media. For instance, \"Modi promised...\" starts with a noun, which is grammatically correct but often in more formal writing, one might see a determiner or adverb preceding it.\n",
        "\n",
        "2.  **`WRB` (Wh-adverb) Transitioning to Nouns (`NN`) or Adjectives (`JJ`):**\n",
        "    *   `WRB`: `{'NN': 0.1935, 'JJ': 0.1613, 'VBP': 0.0968, 'RB': 0.0968, 'VBZ': 0.0645}`\n",
        "    *   It's somewhat unusual to see a `WRB` (like 'why', 'when', 'how') directly followed by a Noun or Adjective with such high probability. In standard English, `WRB` is typically followed by a verb or auxiliary verb to form a question or subordinate clause. However, in informal social media, incomplete sentences or direct exclamations like \"Why Modi?\" or \"How good!\" could contribute to this pattern.\n",
        "\n",
        "3.  **`NN` (Noun) Transitioning to another `NN`:**\n",
        "    *   `NN`: `{'NN': 0.3601, 'IN': 0.1043, 'JJ': 0.0619, 'NNS': 0.0609, 'CC': 0.0545}`\n",
        "    *   The highest probability for a Noun to be followed by another Noun (36%) is notable. This could indicate frequent use of noun phrases, compound nouns (e.g., \"government maximum governance\"), or appositives, which are often condensed in social media. Examples might include"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd4002cd"
      },
      "source": [
        "### Analysis of Transition Probability Irregularities\n",
        "\n",
        "Upon examining the calculated transition probabilities, several patterns emerge that reflect the nature of social media text, often deviating from standard English grammar. Here are some observations:\n",
        "\n",
        "1.  **`<s>` (Start of Sentence) Transitioning Directly to Nouns (`NN`) or Adjectives (`JJ`):**\n",
        "    *   `<s>`: `{'NN': 0.38, 'JJ': 0.11, 'RB': 0.1, 'DT': 0.06, 'IN': 0.06}`\n",
        "    *   A high probability of a sentence starting directly with a Noun (NN, 38%) or Adjective (JJ, 11%) is common in headlines, short phrases, or sentences where articles/pronouns are omitted for brevity, characteristic of social media. For instance, \"Modi promised...\" starts with a noun, which is grammatically correct but often in more formal writing, one might see a determiner or adverb preceding it.\n",
        "\n",
        "2.  **`WRB` (Wh-adverb) Transitioning to Nouns (`NN`) or Adjectives (`JJ`):**\n",
        "    *   `WRB`: `{'NN': 0.1935, 'JJ': 0.1613, 'VBP': 0.0968, 'RB': 0.0968, 'VBZ': 0.0645}`\n",
        "    *   It's somewhat unusual to see a `WRB` (like 'why', 'when', 'how') directly followed by a Noun or Adjective with such high probability. In standard English, `WRB` is typically followed by a verb or auxiliary verb to form a question or subordinate clause. However, in informal social media, incomplete sentences or direct exclamations like \"Why Modi?\" or \"How good!\" could contribute to this pattern.\n",
        "\n",
        "3.  **`NN` (Noun) Transitioning to another `NN`:**\n",
        "    *   `NN`: `{'NN': 0.3601, 'IN': 0.1043, 'JJ': 0.0619, 'NNS': 0.0609, 'CC': 0.0545}`\n",
        "    *   The highest probability for a Noun to be followed by another Noun (36%) is notable. This often indicates the frequent use of compound nouns or noun phrases without intervening prepositions or determiners (e.g., \"government maximum governance\", \"election campaign\"), which is a common feature in concise social media communication.\n",
        "\n",
        "4.  **`NNP` (Proper Noun, Singular) Transitioning to `NN` or `JJ`:**\n",
        "    *   `NNP`: `{'NN': 0.5625, 'JJ': 0.125, 'RB': 0.125, 'VBD': 0.0625, 'NNS': 0.0625}`\n",
        "    *   A very high probability of an `NNP` being followed by an `NN` (56.25%) is expected for proper noun phrases (e.g., \"New York City\"). However, the POS tagger sometimes misclassifies non-proper nouns or even punctuation as `NNP` (e.g., '“' and '”' were tagged `NNP` in the sample output). If such misclassifications occur, this could lead to unusual but statistically frequent transitions, like `NNP` (punctuation) followed by `JJ` or `NN`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "123a9811",
        "outputId": "419aa575-6eb1-44ac-d31a-82e399ddec0a"
      },
      "source": [
        "emission_probabilities = defaultdict(dict)\n",
        "\n",
        "for tag, words_counter in emission.items():\n",
        "    total_count = sum(words_counter.values())\n",
        "    for word, count in words_counter.items():\n",
        "        emission_probabilities[tag][word] = count / total_count if total_count > 0 else 0\n",
        "\n",
        "print(\"\\nCalculated Emission Probabilities (first 5 POS tags):\")\n",
        "# Sort the inner dictionaries by probability in descending order for better analysis\n",
        "for tag, word_probs in list(emission_probabilities.items())[:5]:\n",
        "    sorted_word_probs = dict(sorted(word_probs.items(), key=lambda item: item[1], reverse=True))\n",
        "    print(f\"  {tag}: {dict(list(sorted_word_probs.items())[:5])}\") # Print top 5 for each tag\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculated Emission Probabilities (first 5 POS tags):\n",
            "  WRB: {'why': 0.41935483870967744, 'when': 0.25806451612903225, 'how': 0.16129032258064516, 'where': 0.16129032258064516}\n",
            "  NN: {'modi': 0.10299003322259136, 'india': 0.023255813953488372, 'vote': 0.022425249169435217, 'bjp': 0.011627906976744186, 'time': 0.009136212624584718}\n",
            "  VBD: {'was': 0.14912280701754385, 'did': 0.07017543859649122, 'were': 0.06140350877192982, 'kar': 0.05263157894736842, 'said': 0.02631578947368421}\n",
            "  NNP: {'’': 0.5, '”': 0.125, '‘': 0.125, '“': 0.0625, '₹': 0.0625}\n",
            "  JJ: {'modi': 0.051685393258426963, 'narendra': 0.033707865168539325, 'indian': 0.02247191011235955, 'other': 0.020224719101123594, 'great': 0.01348314606741573}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc077a3b"
      },
      "source": [
        "### Analysis of Rare Tokens and Out-Of-Vocabulary (OOV) Terms\n",
        "\n",
        "Having calculated the emission probabilities, we can now identify rare tokens and discuss the challenges posed by Out-Of-Vocabulary (OOV) terms, especially in the context of social media:\n",
        "\n",
        "#### Identifying Rare Tokens:\n",
        "To identify rare tokens, we look for words with very low emission probabilities for a given POS tag. These are often words that appear only once or twice (hapax legomena or dis legomena) within the training data for that specific tag.\n",
        "\n",
        "Let's consider a few examples from the computed `emission_probabilities`:\n",
        "\n",
        "*   **For `NN` (Noun) tag:**\n",
        "    While words like 'modi' have a high probability (0.102), many words at the tail end of the `NN` emission list would have extremely low probabilities. For instance, words like 'nonsense', 'drama', 'campaigner', 'confusion', 'putin', 'grace', 'smriti', 'hema', 'introspect', 'saga', 'pair', 'brexit' (as seen in the original `emission` dictionary) likely have very low individual probabilities for the 'NN' tag because they appear infrequently. A word appearing only once for a given tag would have an emission probability of `1 / total_count_for_tag`.\n",
        "\n",
        "*   **For `JJ` (Adjective) tag:**\n",
        "    Similarly, while 'modi', 'narendra', 'indian' have higher probabilities, words like 'minimum', 'difficult', 'refresh', 'maarkefir', 'crustal', 'filthy', 'nonsensical', 'powerful' (from the full `emission` data for `JJ`) would represent rare tokens, each with a very low probability.\n",
        "\n",
        "*   **Punctuation as `NNP`:**\n",
        "    A peculiar observation is the high probability of punctuation marks like `’`, `”`, `‘`, `“` (apostrophe, double quotes) and `₹` (Rupee symbol) for the `NNP` (Proper Noun, Singular) tag. This is a clear misclassification by the POS tagger, where these symbols are treated as proper nouns due to their unique occurrence patterns or perhaps an anomaly in the training data it was built on. While not 'rare' in the sense of appearing infrequently as words, their classification as `NNP` is an irregularity that leads to unexpected high emission probabilities for these non-word tokens.\n",
        "\n",
        "#### Out-Of-Vocabulary (OOV) Terms and HMM Challenges:\n",
        "\n",
        "An HMM model assigns zero probability to any word it encounters during prediction that was not present in its training data for a specific POS tag. In the context of social media text, this poses significant challenges:\n",
        "\n",
        "1.  **Dynamic Vocabulary:** Social media is highly dynamic, with new slang, hashtags, acronyms, and trending terms emerging constantly. An HMM trained on a static corpus will inevitably encounter many OOV words.\n",
        "2.  **Informal Language:** Misspellings, abbreviations, and creative word usage are common, leading to words that might not be in a standard dictionary or a carefully curated training corpus.\n",
        "3.  **Proper Nouns:** New names of people, places, brands, or events appear frequently. If these are not in the training data, they become OOV, making it difficult for the HMM to tag them correctly (e.g., as NNP).\n",
        "4.  **Impact on Tagging:** When an HMM encounters an OOV word, it typically cannot assign it a POS tag with any confidence (or assigns a default, often incorrect, tag like 'NN' or 'UNK' if smoothing is applied). This can propagate errors, as the tag of the current word influences the probability of the next word's tag (via transition probabilities).\n",
        "\n",
        "**Mitigation Strategies (beyond basic HMM):**\n",
        "*   **Smoothing:** Techniques like Laplace smoothing (add-one smoothing) or more advanced methods can assign a small, non-zero probability to OOV words. However, this is a heuristic and doesn't genuinely 'understand' the word.\n",
        "*   **Lexical Guessing:** Using morphological analysis (prefixes, suffixes) or word shape (e.g., capitalization, presence of numbers) to guess the POS of OOV words.\n",
        "*   **Hybrid Models:** Combining HMMs with neural networks or other models that can learn word embeddings, which can handle words not explicitly seen in training by leveraging semantic similarity.\n",
        "\n",
        "In summary, the presence of numerous low-probability emission words and the inherent problem of OOV terms in dynamic social media text highlight the limitations of a purely HMM-based approach for robust POS tagging in such domains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9653106",
        "outputId": "61b97fe0-2ae0-4f45-9903-b3730e36eb8e"
      },
      "source": [
        "#Manually Apply Viterbi Decoding\n",
        "sample_tweet_text = cleaned.iloc[2] # Choosing the third cleaned tweet as an example\n",
        "sample_tweet_tokens = nltk.word_tokenize(sample_tweet_text)\n",
        "\n",
        "print(f\"Selected Sample Tweet: {sample_tweet_text}\")\n",
        "print(f\"Tokenized Sample Tweet: {sample_tweet_tokens}\")\n",
        "\n",
        "# Initialize dictionaries to store Viterbi probabilities and backpointers\n",
        "viterbi_probabilities = defaultdict(lambda: defaultdict(float))\n",
        "viterbi_backpointers = defaultdict(lambda: defaultdict(str))\n",
        "\n",
        "# Get all unique tags from our transition probabilities for reference\n",
        "all_tags = list(transition_probabilities.keys())"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Sample Tweet: what did just say vote for modi  welcome bjp told you rahul the main campaigner for modi think modi should just relax\n",
            "Tokenized Sample Tweet: ['what', 'did', 'just', 'say', 'vote', 'for', 'modi', 'welcome', 'bjp', 'told', 'you', 'rahul', 'the', 'main', 'campaigner', 'for', 'modi', 'think', 'modi', 'should', 'just', 'relax']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "199f7710",
        "outputId": "63c9a283-6a9e-42be-bc11-1e71d05b24ea"
      },
      "source": [
        "first_word = sample_tweet_tokens[0].lower()\n",
        "default_prob = 1e-6 # Small probability for unknown transitions/emissions\n",
        "\n",
        "# Step 3: Calculate probabilities for the first word\n",
        "for tag in all_tags:\n",
        "    if tag == '<s>':\n",
        "        continue # '<s>' is a start symbol, not a tag for a word\n",
        "\n",
        "    # Get transition probability from start state '<s>' to current tag\n",
        "    trans_prob = transition_probabilities['<s>'].get(tag, default_prob)\n",
        "\n",
        "    # Get emission probability of the first word given the current tag\n",
        "    # Handle OOV words: if word not in emission_probabilities[tag], assign default_prob\n",
        "    emit_prob = emission_probabilities[tag].get(first_word, default_prob)\n",
        "\n",
        "    viterbi_probabilities[0][tag] = trans_prob * emit_prob\n",
        "    viterbi_backpointers[0][tag] = '<s>' # The previous state was always '<s>'\n",
        "\n",
        "print(f\"\\nViterbi Probabilities for first word ('{first_word}'):\")\n",
        "sorted_first_word_probs = dict(sorted(viterbi_probabilities[0].items(), key=lambda item: item[1], reverse=True))\n",
        "print(dict(list(sorted_first_word_probs.items())[:5])) # Print top 5 for brevity"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Viterbi Probabilities for first word ('what'):\n",
            "{'WP': 0.006923076923076923, 'NN': 3.7999999999999996e-07, 'WDT': 1.111111111111111e-07, 'JJ': 1.0999999999999999e-07, 'RB': 1e-07}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74b57405",
        "outputId": "c39f9e84-8219-4ab4-c100-949b6262069e"
      },
      "source": [
        "for i in range(1, len(sample_tweet_tokens)): # Start from the second word (index 1)\n",
        "    current_word = sample_tweet_tokens[i].lower()\n",
        "\n",
        "    for current_tag in all_tags:\n",
        "        if current_tag == '<s>': # '<s>' is a start symbol, not a tag for a word\n",
        "            continue\n",
        "\n",
        "        max_prob_path = 0.0\n",
        "        best_prev_tag = ''\n",
        "\n",
        "        for prev_tag in all_tags:\n",
        "            if prev_tag == '<s>': # '<s>' is a start symbol, not a tag for a word\n",
        "                continue\n",
        "\n",
        "            # Get previous Viterbi probability\n",
        "            prev_viterbi_prob = viterbi_probabilities[i-1].get(prev_tag, 0.0)\n",
        "\n",
        "            if prev_viterbi_prob == 0.0: # If previous state had zero probability, skip\n",
        "                continue\n",
        "\n",
        "            # Get transition probability from previous tag to current tag\n",
        "            trans_prob = transition_probabilities[prev_tag].get(current_tag, default_prob)\n",
        "\n",
        "            # Get emission probability of current word given current tag\n",
        "            emit_prob = emission_probabilities[current_tag].get(current_word, default_prob)\n",
        "\n",
        "            # Calculate current path probability\n",
        "            current_path_prob = prev_viterbi_prob * trans_prob * emit_prob\n",
        "\n",
        "            if current_path_prob > max_prob_path:\n",
        "                max_prob_path = current_path_prob\n",
        "                best_prev_tag = prev_tag\n",
        "\n",
        "        viterbi_probabilities[i][current_tag] = max_prob_path\n",
        "        viterbi_backpointers[i][current_tag] = best_prev_tag\n",
        "\n",
        "    print(f\"\\nViterbi Probabilities for word {i+1} ('{current_word}'):\")\n",
        "    sorted_word_probs = dict(sorted(viterbi_probabilities[i].items(), key=lambda item: item[1], reverse=True))\n",
        "    print(dict(list(sorted_word_probs.items())[:5])) # Print top 5 for brevity"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Viterbi Probabilities for word 2 ('did'):\n",
            "{'VBD': 0.00011211460604173155, 'VBP': 1.242603550295858e-09, 'VBZ': 7.10059171597633e-10, 'NN': 5.325443786982249e-10, 'JJ': 5.325443786982249e-10}\n",
            "\n",
            "Viterbi Probabilities for word 3 ('just'):\n",
            "{'RB': 5.168485645883487e-07, 'NN': 2.182762241520437e-11, 'JJ': 2.182762241520437e-11, 'DT': 8.92948189712906e-12, 'NNS': 5.952987931419374e-12}\n",
            "\n",
            "Viterbi Probabilities for word 4 ('say'):\n",
            "{'VBP': 1.0028538033147504e-09, 'VB': 5.14106662388941e-10, 'JJ': 1.0187880359674181e-13, 'RB': 6.212122170533037e-14, 'IN': 3.7272733023198224e-14}\n",
            "\n",
            "Viterbi Probabilities for word 5 ('vote'):\n",
            "{'NN': 3.6272978096530734e-12, 'VB': 9.56161261701653e-13, 'VBP': 3.1085960301959574e-13, 'JJ': 1.9410073612543554e-16, 'VBN': 1.155361524556164e-16}\n",
            "\n",
            "Viterbi Probabilities for word 6 ('for'):\n",
            "{'IN': 1.0834108299961867e-13, 'NN': 1.3062291281299155e-18, 'JJ': 2.24403465601806e-19, 'NNS': 2.2105416014506265e-19, 'CC': 1.9760902194785903e-19}\n",
            "\n",
            "Viterbi Probabilities for word 7 ('modi'):\n",
            "{'NN': 3.836958502619353e-15, 'JJ': 8.631873824280139e-16, 'NNS': 3.005092101785415e-16, 'PDT': 1.5571840891069877e-16, 'FW': 1.0705640612610539e-16}\n",
            "\n",
            "Viterbi Probabilities for word 8 ('welcome'):\n",
            "{'NN': 1.1476164524082144e-18, 'JJ': 5.33425087772203e-19, 'IN': 4.0034747072574966e-22, 'NNS': 2.33831266087606e-22, 'CC': 2.0903098029043567e-22}\n",
            "\n",
            "Viterbi Probabilities for word 9 ('bjp'):\n",
            "{'NN': 4.805454448659018e-21, 'JJ': 6.381798567138753e-22, 'VBZ': 4.6822376679241715e-22, 'NNS': 3.027069594519156e-22, 'VB': 1.9000877360214966e-22}\n",
            "\n",
            "Viterbi Probabilities for word 10 ('told'):\n",
            "{'VBD': 2.140739617665727e-24, 'NN': 1.7304960618439677e-27, 'IN': 5.014001409958163e-28, 'JJ': 2.972903490860149e-28, 'NNS': 2.9285317969667142e-28}\n",
            "\n",
            "Viterbi Probabilities for word 11 ('you'):\n",
            "{'PRP': 4.7361495966055905e-26, 'NN': 4.167811645012919e-31, 'JJ': 4.167811645012919e-31, 'DT': 1.7050138547780124e-31, 'RB': 1.3261218870495653e-31}\n",
            "\n",
            "Viterbi Probabilities for word 12 ('rahul'):\n",
            "{'VBP': 9.102145284956291e-29, 'VB': 3.2663100666245456e-29, 'VBZ': 7.867358133896329e-30, 'JJ': 1.5204332573372684e-30, 'MD': 8.119113594181013e-33}\n",
            "\n",
            "Viterbi Probabilities for word 13 ('the'):\n",
            "{'DT': 2.629055043297768e-30, 'JJ': 1.761705539023798e-35, 'NN': 1.4680879491864984e-35, 'VBN': 1.0486342494189275e-35, 'IN': 9.227981394886562e-36}\n",
            "\n",
            "Viterbi Probabilities for word 14 ('main'):\n",
            "{'JJ': 1.18159777226866e-33, 'NN': 1.3408180720818616e-36, 'NNS': 2.366149538967991e-37, 'DT': 1.1830747694839956e-37, 'CD': 7.887165129893303e-38}\n",
            "\n",
            "Viterbi Probabilities for word 15 ('campaigner'):\n",
            "{'NN': 5.745287679508806e-37, 'NNS': 1.911012342393505e-40, 'JJ': 1.238120672536637e-40, 'IN': 2.9607233473702187e-41, 'RB': 2.1532533435419773e-41}\n",
            "\n",
            "Viterbi Probabilities for word 16 ('for'):\n",
            "{'IN': 1.7160176032027632e-38, 'NN': 2.0689401615959687e-43, 'JJ': 3.5543330981264076e-44, 'NNS': 3.501283350393178e-44, 'CC': 3.129935116260568e-44}\n",
            "\n",
            "Viterbi Probabilities for word 17 ('modi'):\n",
            "{'NN': 6.077369868341172e-40, 'JJ': 1.3672050362596069e-40, 'NNS': 4.75977422703769e-41, 'PDT': 2.4664284631013486e-41, 'FW': 1.695669568382177e-41}\n",
            "\n",
            "Viterbi Probabilities for word 18 ('think'):\n",
            "{'VBP': 2.7177287214176144e-43, 'NN': 1.8177130775631376e-43, 'VB': 5.031095506634523e-44, 'IN': 6.341115375092819e-47, 'JJ': 3.759776372842645e-47}\n",
            "\n",
            "Viterbi Probabilities for word 19 ('modi'):\n",
            "{'NN': 6.74150219727358e-45, 'JJ': 2.7187137626359608e-45, 'NNS': 4.921749806755159e-46, 'VBP': 3.650521107068359e-46, 'VB': 3.0230575321664233e-46}\n",
            "\n",
            "Viterbi Probabilities for word 20 ('should'):\n",
            "{'MD': 2.7665957514203672e-47, 'NN': 2.427687771871372e-51, 'IN': 7.034069697986283e-52, 'NNS': 4.397008591051326e-52, 'JJ': 4.170643095266204e-52}\n",
            "\n",
            "Viterbi Probabilities for word 21 ('just'):\n",
            "{'RB': 2.8998055697822392e-49, 'VB': 2.0262391418853395e-53, 'PRP': 7.793227468789766e-55, 'VBG': 7.793227468789766e-55, 'WRB': 3.896613734394883e-55}\n",
            "\n",
            "Viterbi Probabilities for word 22 ('relax'):\n",
            "{'VB': 2.8844219858576644e-52, 'JJ': 5.715962901974606e-56, 'RB': 3.4853432329113453e-56, 'VBP': 2.5094471276961683e-56, 'IN': 2.0912059397468066e-56}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1b065c2",
        "outputId": "c25f6966-d1df-48ef-ad8f-342e9a068752"
      },
      "source": [
        "final_tags = []\n",
        "\n",
        "# Find the tag with the maximum probability for the last word\n",
        "last_word_index = len(sample_tweet_tokens) - 1\n",
        "\n",
        "if last_word_index >= 0 and viterbi_probabilities[last_word_index]:\n",
        "    max_final_prob = 0.0\n",
        "    best_final_tag = ''\n",
        "\n",
        "    for tag, prob in viterbi_probabilities[last_word_index].items():\n",
        "        if prob > max_final_prob:\n",
        "            max_final_prob = prob\n",
        "            best_final_tag = tag\n",
        "\n",
        "    # Backtrack to find the full sequence of tags\n",
        "    current_tag = best_final_tag\n",
        "    for i in range(last_word_index, -1, -1):\n",
        "        final_tags.insert(0, current_tag)\n",
        "        current_tag = viterbi_backpointers[i][current_tag]\n",
        "else:\n",
        "    print(\"Could not perform backtracking: no probabilities calculated or empty tweet.\")\n",
        "\n",
        "print(\"\\n--- Viterbi Decoding Result ---\")\n",
        "print(f\"Sample Tweet: {sample_tweet_text}\")\n",
        "print(\"Tokenized Tweet and Predicted POS Tags:\")\n",
        "for i in range(len(sample_tweet_tokens)): # Ensure both lists have the same length\n",
        "    if i < len(final_tags):\n",
        "        print(f\"  {sample_tweet_tokens[i]}: {final_tags[i]}\")\n",
        "    else:\n",
        "        print(f\"  {sample_tweet_tokens[i]}: [TAG_MISSING]\")\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Viterbi Decoding Result ---\n",
            "Sample Tweet: what did just say vote for modi  welcome bjp told you rahul the main campaigner for modi think modi should just relax\n",
            "Tokenized Tweet and Predicted POS Tags:\n",
            "  what: WP\n",
            "  did: VBD\n",
            "  just: RB\n",
            "  say: VBP\n",
            "  vote: NN\n",
            "  for: IN\n",
            "  modi: NN\n",
            "  welcome: NN\n",
            "  bjp: NN\n",
            "  told: VBD\n",
            "  you: PRP\n",
            "  rahul: VB\n",
            "  the: DT\n",
            "  main: JJ\n",
            "  campaigner: NN\n",
            "  for: IN\n",
            "  modi: NN\n",
            "  think: NN\n",
            "  modi: NN\n",
            "  should: MD\n",
            "  just: RB\n",
            "  relax: VB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ccb3a9f"
      },
      "source": [
        "## Discuss HMM Challenges with Social Media Text\n",
        "\n",
        "### Subtask:\n",
        "Based on the analysis, discuss the inherent difficulties and limitations of Hidden Markov Models (HMMs) when applied to the informal, dynamic, and often grammatically unconventional nature of social media text.\n",
        "\n",
        "#### Discussion of HMM Challenges with Social Media Text\n",
        "\n",
        "Based on the analysis of transition probabilities and emission probabilities, several inherent difficulties and limitations of Hidden Markov Models (HMMs) become apparent when applied to the informal, dynamic, and often grammatically unconventional nature of social media text:\n",
        "\n",
        "1.  **Irregular Transition Patterns and Grammatical Deviations:**\n",
        "    *   **Start of Sentence (`<s>`) Transitions:** We observed high probabilities of sentences starting directly with Nouns (`NN`) or Adjectives (`JJ`), and `WRB` (Wh-adverb) followed by Nouns or Adjectives. While some are legitimate (e.g., compound nouns), others represent abbreviated, exclamatory, or headline-style constructs common in social media. HMMs rely on learned grammatical patterns. When these patterns are inconsistent or frequently deviate from standard English (as seen in social media), the transition probabilities can become skewed, leading to incorrect tag sequences for more conventional sentence structures.\n",
        "    *   **High Noun-Noun (`NN` to `NN`) Transition:** The high probability of `NN` followed by `NN` reflects the prevalence of compound nouns or condensed noun phrases. While an HMM can learn this, it might struggle to differentiate between legitimate compounds and instances where other parts of speech (like determiners or prepositions) are omitted for brevity, leading to misinterpretations of the sentence's grammatical structure.\n",
        "\n",
        "2.  **Out-Of-Vocabulary (OOV) Terms and Rare Tokens:**\n",
        "    *   **Dynamic Vocabulary:** Social media language is characterized by rapid evolution, with new slang, abbreviations, hashtags, proper nouns, and trending terms emerging constantly. An HMM's emission probabilities are directly learned from its training corpus. If a word encountered during tagging was not present in the training data (an OOV term) or appeared very rarely, the model assigns it a very low (or zero) emission probability.\n",
        "    *   **Impact on Tagging:** When an HMM encounters an OOV word, it cannot assign a tag with confidence. In our Viterbi implementation, a `default_prob` (1e-6) was used for unknown transitions or emissions. This is a heuristic that prevents zero probabilities from halting the algorithm but doesn't genuinely 'understand' the word. This uniform low probability can lead to arbitrary tag assignments for OOV words, which then propagate errors through the sequence due to the reliance on previous tags (via transition probabilities).\n",
        "    *   **Examples from Analysis:** Words like 'modi' or 'bjp' might appear frequently, but many domain-specific terms, misspellings, or unique user-generated content will be rare or OOV. For instance, the misclassification of punctuation (`’`, `”`, `‘`, `“`, `₹`) as `NNP` (Proper Noun) is an artifact of the training data or tagger's rules, highlighting how unusual tokens can challenge standard POS tagging, even if they aren't strictly OOV for the tagger.\n",
        "\n",
        "3.  **Limitations of `default_prob`:**\n",
        "    *   The `default_prob` is a form of smoothing, a critical technique to handle unseen events. However, it's a blunt instrument. It treats all unseen words (given a tag) or unseen tag transitions equally. A more robust model would need to employ more sophisticated smoothing techniques (e.g., Kneser-Ney smoothing) or incorporate external knowledge (e.g., word embeddings, character-level features) to make more informed guesses about OOV words.\n",
        "    *   For social media, where OOV is frequent, a simple `default_prob` means the model is essentially guessing, undermining the statistical power of the HMM.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "While HMMs provide a foundational understanding of sequential data, their reliance on discrete states and observable emissions from a fixed vocabulary makes them less suitable for the complexities of social media text. The informality, dynamic nature, and grammatical unconventionality lead to:\n",
        "*   **Inaccurate Transition Probabilities:** Learned patterns don't always reflect the intended grammar.\n",
        "*   **High OOV Rates:** Frequent encounters with unknown words result in arbitrary tag assignments.\n",
        "*   **Propagation of Errors:** Mistakes with one word can cascade through the entire sequence.\n",
        "\n",
        "Without significant modifications, such as more advanced smoothing, integrating neural word embeddings (as seen in hybrid models), or domain-specific training data, HMMs will struggle to achieve high accuracy and reliability for POS tagging in such dynamic and unconventional environments."
      ]
    }
  ]
}